\documentclass\cite{areview}{elsarticle}

\usepackage{lineno,hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subfigure}
\usepackage{mathtools}
\usepackage{graphicx}
%\usepackage{rotating}

\modulolinenumbers\cite{a5}

\journal{Journal of \LaTeX\ Templates}


\bibliographystyle{elsarticle-num}


\begin{document}

\begin{frontmatter}

\title{Product Recommendation System Using Tournament-Selection Empowered
Whale Optimization Algorithm Optimized K-Means and MapReduce}

\author{Ashish Kumar Tripathi*}
\cortext\cite{amycorrespondingauthor}{Corresponding author}
\ead{ashish.tripathi@jiit.ac.in}
\author{Siddharth Gupta}
\author{Pranav Saxena}
\begin{abstract}
 In the era of Web 2.0, the data is growing on an immense scale. The E-commerce business require information retrieval tools like Recommender Systems, or assisting their costumers to select better choice for their shopping bucket. One of the prominent recommendation system approach is Collaborative filtering, that outputs best suggestion by finding similar items based on user’s shopping history. K-Means prominent method is used to find clusters  of similar items in Collaborative filtering. K-Means is a greedy approach and thus suffers from local minima problem. To mitigate this problem,  nature-inspired algorithm named Tournament-Selection Empowered Whale Optimization Algorithm Optimized K-Means (TSWOAK) is introduced. The introduced  algorithm TSWOAK, uses the hunting behaviour of Humpback Whales and Tournament-Selection concept to achieve global optima. The introduced algorithm is  tested on seven benchmark UCI datasets, and the results are compared with the ones achieved by state-of-art algorithms namely, Bat algorithm, Particle  Swarm Optimization, Grey-Wolf Optimization, K-Means on the  basis of inter-cluster distance. The results showed that the K-Means has been optimized by the proposed method. The algorithm was then checked for its application in Recommender System. For checking its applicability, it was tested on  MovieLens dataset. The results in term of Mean Absolute Error(MAE) were compared to the results of already present algorithms namely, K-Means,  PCA-K-Means, ABC-KM, GAKM, PCA-GAKM, SOM, PCA-SOM, UPCC. The comparison shows that the TSWOAK has lesser MAE than other algorithms, which shows that  Recommender System is a good application of TSWOAK. The problem which TSWOAK still face is that it will fail on large datasets, as the time complexity of nature-inspired algorithms is high. To solve this problem, the TSWOAK is adapted in the MapReduce model of Apache-Hadoop framework and is named MR-TSWOAK.
\end{abstract}

\begin{keyword}
Clustering method  \sep MapReduce \sep Recommendation System \sep Tournament-Selection \sep and Whale Optimization Algorithm 
\end{keyword}

\end{frontmatter}



\section{Introduction} \label{sec:intro}

 Recommendation Systems are one of the most important information retrieval tools used by online businesses to make the environment for customers more  personal. Most promiment technique for Recommendation System is the Collaborative Filtering \cite{a1}. K-Means algorithm is one of the most used algorithms in Collaborative Filtering \cite{a4}. K-Means being a greedy approach suffers from trapping in local- optima \cite{a5}. To prevent K-Means  from getting trapped into local-optima, many nature-inspired algorithms have been developed and are hybridized with the clustering algorithm. Maulik et  al. \cite{a7} were first to propose methodology of using nature-inspired algorithm to optimise the positioning of centroids, in a way that the  intra-cluster distance is optimized. Hatamlou et al. \cite{a6} have optimized K-Means algorithm using Gravitational Search Algorithm (GSA). The GSA  as used to optimize the repositioning of cluster- centroids. Sharma et al. \cite{a8} have proposed to use bat-algorithm to optimize the positioning of  cluster’s centroids and have further extend their method on MapReduce. Pandey et al. \cite{a9} have proposed an algorithmthat uses Hybrid Cuckoo  search Algorithm to optimize clustering of twitter data being used in sentiment analysis of tweets. Cura et al. \cite{a10} have proposed a method to  optimize the clustering of sensor’s data using Particle Swarm Optimization. Though the above-mentioned methods perform good on small datasets, but  fail on large datasets, as the nature-inspired algorithm-based clustering algorithms have high time complexity \cite{a5}. Most of the Recommendation  systems suffer from reduced scalability \cite{a2}. To solve these problems, Apache-Hadoop needs to be braught into application, as it is an efficient  tool for parallel- computing. The tool uses MapReduce programming model \cite{a11} which helps to parallelize the tasks on  DataNodes \cite{a12} Apache-Hadoop is a state-of-art open-source tool for handling large datasets, and support parallel- computing. Hadoop uses its own file-system called  as Hadoop Distributed File System (HDFS) \cite{a12}. The HDFS is a master-slave architecture, in which there exist one master node called as NameNode,   which keeps the meta-data of all the slave nodes called as DataNodes. MapReduce programming model \cite{a11} is used by Hadoop for  parallel-processing. The MapReduce model has a Map function and Reduce function.  The Map function takes <key, value> pair as an input and outputs intermediate <key, value> pairs. The  Reduce function takes all the intermediate <key, value> pairs and combine all values related to each unique key.  As Hadoop simplifies parallel-computing, many meta-heuristic algorithms have been developed over Hadoop. MapReduce-based Artificial Bee Colony  Algorithm \cite{a13} optimised the clustering of large datasets. Dynamic Frequency based K-Bat algorithm (DFBPKBA) \cite{a14} showed that the dynamic   change to frequency of bats optimised the clustering process and the MapReduce version of the proposed method was able to handle large datasets.  Tripathi et al. \cite{a5} have proposed an algorithm that uses a hybrid of Grey-wolf Optimizer with Levy Flights and Crossover on MapReduce programming  model. Whale Optimization Algorithm \cite{a15} is a meta-heuristic nature inspired algorithm inspired by the hunting  behaviour of  Humpback whales. The algorithm has shown better results than many state of art algorithms such as Particle-swarm Optimization. Grey-wolf Optimizer, Differential Evolution, Gravitational Search Algorithm, Fast Evolutionary Programming. Though Whale Optimization Algorithm has proved to give better results than other state of the art meta-heuristic nature inspired algorithms, but it has risk to get into local optima \cite{a16}. Whale Optimization Algorithm discard the (called as whales) with bad fitness values, and there is a chance that the whale having bad fitness value might be near  global-optima \cite{a16}. To overcome this shortcoming, the authors have proposed a method that uses the concept of Tournament Selection along with  WOA, called as Tournament Selection Empowered Whale Optimization Algorithm optimized K-Means (TSWOAK). To handle extremely large dataset, the authors  have further proposed a MapReduce variant of TSWOAK called MapReduce-based Tournament Selection Empowered Whale Optimization Algorithm optimized K-Means (MR-TSWOAK). The proposed method TSWOAK has been tested on seven UCI datasets and the results  have been compared with respect to  intra-cluster distance with state of art algorithm namely. K-Means, Bat Algorithm, Gravitation Search Algorithm, Particle Swarm Optimization, and  grey-wolf Optimizer. The  UCI datasets used are Iris, Seeds, Glass, Cancer, Balance, Haberman, and Wine. The MapReduce based method MR-TSWOAK was  tested on large datasets namely, Replicated Iris, Replicated CMC, Replicated Wine, Replicated Vowel, and the results were compared with respect to  intra-cluster distance with popular MapReduce based meta-heuristic algorithms namely, Parallel K-Means Algorithm, Parallel K-PSO Algorithm, Dynamic  frequency based parallel K-Bat (DFBPKB) Algorithm. Further the authors, tested the  proposed method for Recommendation System application. For this  test, MovieLens dataset was used. The result of the test was compared with respect to Mean Absolute Error with state of art collaborative filtering algorithms namely, PCA-GAKM, PCA-SOM, SOM, UPCC, K-Means, PCA-K-Means, GAKM, ABC-KM.The research paper has been organized as: Section \ref{sec:bg} discusses the basics of data-clustering and Whale Optimization Algorithm (WOA). Section \ref{sec:pm} discusses the clustering method of the proposed  method TSWOAK, and further discusses the MapReduce-based proposed method MR-TSWOAK. Section \ref{sec:ep} presents the experimental arrangements and results. And Section \ref{sec:con} gives the conclusion of the paper.


\section{Background}\label{sec:bg}

     \subsection{Data Clustering Approach}
Data clustering is an unsupervised machine learning approach. Clustering algorithm checks for similar-
looking data-points and group them in same cluster. Clustering in K clusters of N data-points is an iterative
process. There is no training required unlike supervised approaches like regression and classification etc.
Clustering is done in such a way that the sum of Euclidean distances of all data-points with thier
corresponding centroid is minimum. Let $Z=\{\{z_1_1 ,z_1_2 ,...,z_1_t\},\{z_2_1 ,z_2_2,...,z_2_t\},...,\{z_n_1 ,z_n_2 ,...,z_n_t\}\}$ be a set of
‘N’ data-points having ‘t’ features each, where the value of $i^t^h$ data-point's $j^t^h$ attribute is denoted by ‘$z_i_j $’.
The clustering algorithm runs iterativelyand finds set of cluster-centroids, $C=\{\{c_1_1 ,c_1_2 ,...,c_1_t \},\{c_2_1 ,c_2_2 ,...,c_2_t \} ...,\{c_k_1 ,c_k_2 ,...,c_k_t \}\}$, where the value of $j^t^h$ attribute of $i^t^h$ centroid is denoted by ‘$c_i_j$’. The set $C_i = \{c_i_1 ,c_i_2 ,...,c_i_t \}$ is position vector of i th cluster-centroid. The set C is found such
that the intra-cluster distance is minimum. Any clustering algorithm should satisfy the given conditions:   

\begin{itemize}
\item Each cluster should contain at least one data-point, i.e., $C_i \neq \phi , \forall i\in \{1,2,3, \cdots,k\}$.
\item No data-point can be outside of all clusters
\item There should be no data-point belonging to more than one cluster, i.e., $C_q \cap C_r=\phi,\forall q\neq r$ and $q,r \in \{1,2,3,\cdots,k\}$.
\end{itemize}   
Clustering algorithm are tested on the basis of intra-cluster distance. The intra-cluster distance is the
summation of Euclidean distance of all data-points with their corresponding cluster-centroid. The intra-
cluster distance is calculated as given in Eq. (\ref{eq:25}).
   \begin{equation}\label{eq:25} 
   f(Z,C)=\sum_{l=1}^k\sum_{Z_i\epsilon C_l}d(Z_i,C_l)^2
   \end{equation}
Where $Z_i$ is a data-point belonging to $C_i$ . $d\(Z_i ,C_i \)$ is called as Euclidean distance between $Z_i$ and its
corresponding cluster-centroid $C_i$ . The Euclidean distance is calculated as given in Eq. (\ref{eq:26}).
   \begin{equation}\label{eq:26}
     d(Z_i,Z_j)=\sqrt{\sum_{t=1}^t(z_i_j-z_i_j)^2}
   \end{equation}
  \subsection{Whale Optimization Algorithm}\label{sec:GWO}
 Whale Optimization Algorithm \cite{a15} is inspired by the hunting nature of humpback whales. The algorithm
works in exploration phase and exploitation phase. The Exploitation phase is further divided into two
different strategies, namely, Shrinking encircling mechanism and Spiral update mechanism. In Shrinking
encircling mechanism, the whale moves toward the best whale of population in circular manner. It can be
mathematically defined by Eq. (\ref{eq:3}).


    
 \begin{equation}\label{eq:3}                               
   \overrightarrow X(t+1)=\overrightarrow{X^*}(t)+\overrightarrow A.\overrightarrow D
\end{equation}
\begin{equation}\label{eq:4}
    \overrightarrow D=|\overrightarrow C.\overrightarrow{X^*}(t)-\overrightarrow X(t)|                           
\end{equation}
\begin{equation}\label{eq:5}                                             	
       \overrightarrow A=2 \overrightarrow a. \overrightarrow {r}-a.                                                     	
   \end{equation}
   \begin{equation}\label{eq:6} 
   \overrightarrow C=2. \overrightarrow {r}.
    \end{equation}

     where $\overrightarrow{X^*}t(i)$ is the position of best whale in previous iteration, $\overrightarrow {X}(t)$ is a whale position and \lq{t}\rq indicates
the current iteration. The mathematical Shrinking encircling behaviour is achieved by linearly decreasing
the value of |$\overrightarrow{a}$| in Eq. (\ref{eq:4}) from 2 to 0 over the course of iterations and |\overrightarrow{r}| is a random number uniformly distributed in the range of [0,1]. In Spiral update mechanism, the distance between best whale position  ($\overrightarrow{x^*}(t))$ and the position of whale ($\overrightarrow{X}$(t)). Helix-movement of the whale is then mathematically mimicked by the Eq. (\ref{eq:7}).

     \begin{equation}\label{eq:7}
    \overrightarrow X (t+1)=\overrightarrow{D\rq}.e^b^l.cos(2\pi l)+\overrightarrow{X^*}(t)                           
\end{equation}

Where $\overrightarrow{D\rq}=|\overrightarrow{X^*}(t)-\overrightarrow X(t)|$ is the distance between the whale and the best whale. b is a constant for
Where $D\rq$
defining the shape of logarithmic spiral, l is random number between [-1,1]. Both the strategies have 50\%
probability of being used. The mathematical model that is used by the humpback whale is given it Eq. (\ref{eq:8})

 \begin{equation}\label{eq:8}
\overrightarrow{X}(t+1)=
  \begin{cases}
    \overrightarrow{X^*}(t)+\overrightarrow A.\overrightarrow D & \text{if p less 0.5} \\
    \overrightarrow{D\rq}.e^b^l.cos(2\pi l)+\overrightarrow{X^*}(t) & \text{otherwise}
  \end{cases}
\end{equation} 

Where, p is a random number between [0,1].

In exploration phase, the whale chooses a random position $\overrightarrow{X_{rand}}$ and move towards it. The phase is triggered when the value of $|\overrightarrow A| > 1 $ in Eq. (\ref{eq:5}) of Shrinking encircling mechanism. It is shown in
Eq. (\ref{eq:9}) and (\ref{eq:10})

 \begin{equation}\label{eq:9}
       \overrightarrow D=|\overrightarrow C.\overrightarrow{X_{rand}}(t)-\overrightarrow X(t)|                          
\end{equation} 
\begin{equation}\label{eq:10}
        \overrightarrow X(t+1)=\overrightarrow{X_{rand}}(t)+\overrightarrow A.\overrightarrow D     
\end{equation}
      

      \section{Proposed Method}\label{sec:pm}
     Whale Optimization Algorithm discards the solution (called whale in WOA) with bad fitness value. But
there could be a case that global-optima would be near that solution. This could lead WOA to get into the
trap of local-optima. The authors have therefore proposed a method called as Tournament Selection
empowered Whale Optimization Algorithm optimized K-Means. Nature-inspired meta-heuristic algorithms
are iterative and thus have high time complexity. Therefore, the nature-inspired algorithms cannot be
applied on extremely large datasets. Therefore, the authors have further proposed the method over
MapReduce programming model called as MapReduce-based Tournament Selection empowered Whale
Optimized Algorithm optimized K-Means (MR-TSWOAK).

     \subsection{Tournament Selection empowered WOA optimized K-Means (TSWOAK)}  \label{sec:TSWOAK} 
    
    There is a chance for WOA to get into local-optima as it just discards the solution with lower fitness value.
Therefore, the authors have used the concept of Tournament Selection \cite{a17} while choosing a random
solution for Eq. (\ref{eq:9}) and (\ref{eq:10}). Due to the concept of Tournament Selection, each and every solution get
equal chance to compete with other solution to get selected.

\subsection{TSWOAK based clustering}
 The problem of getting trapped in local-optima of clustering algorithm can be solved by using Tournament
Selection empowered Whale Optimization Algorithm optimized K-Means (TSWOAK). In the proposed
method TSWOAK, the position vector $\overrightarrow X$ of each whale is the position of centroid represented by the whale. $\overrightarrow X=\{\{c_1_1,c_1_2,...,c_1_t\},\{c_2_1,c_2_2,...,c_2_t\},...,\{c_k_1,c_k_2,...,c_k_t\}\}$ where $c_i_j$ represents the centroid position for $j^t^h$
feature of $i^t^h$ cluster. Clustering performance is evaluated in terms of intra-cluster distance. The intra-cluster
distance is calculated using formula given in equation 1. The whale with least intra-cluster distance is taken
as best-whale. The algorithm for proposed method is given in Algorithm 1. The running time-complexity
of the proposed method is O(P*N*K*t), where t is the number of iterations for which the algorithm will
run. K is the number of clusters for which the clustering is done. N is the number of data-points. P is the
number of whales used in algorithm

 \begin{algorithm}
%\scriptsize
\caption{:Enhanced Grey Wolf Optimizer based clustering}
\label{algo:EGWO}
\begin{algorithmic}
\STATE\textbf{Input:} $N$ data-points, number of clusters to be used $K$, number of whales to be used $P$.
\STATE\textbf{Ouput:} Best positions of centroids.
\STATE whale population $X_i(i = \{1, 2, ..., P\})$ initialization
\STATE $\overrightarrow {X^*}$ = the best whale of $0^t^h$ iteration
\WHILE{($t$ < maximum iteration possible)}
  \FOR {each whale}
    \STATE Update a, A, C, l, and p
      \IF{$p$<0.5}
        \IF{$|\overrightarrow A|$<1}
          \STATE Position of the current whale is updated by the Eq. (\ref{eq:3})
        \ELSE
          \STATE Select a random whale ($\overrightarrow{X_{rand}}$) by Tournament Selection
          \STATE Position of the current whale is updated by the Eq. (\ref{eq:10})
        \ENDIF
      \ELSE
        \STATE Position of the current whale is updated by the Eq. (\ref{eq:7})
      \ENDIF
  \ENDFOR
  \STATE if available better solution Updata $\overrightarrow {X^*}$
  \STATE  $t=t+1$; 
\ENDWHILE
\STATE Return $\overrightarrow X{\alpha}$  //the position of alpha is the final centroid position
\end{algorithmic}
\end{algorithm}

  \subsection{Parallelization of the EGWO using MapReduce Architecture}\label{sec:mpr}
The running time-complexity of the algorithm TSWOAK is O(P*N*K*t). The time-complexity shows that
it is directly proportional to number of data-points involved. For large number of data-points, the time-
complexity will be become very high. For performing clustering on large datasets, parallel-computing is
required. For parallel-computing, the authors have developed TSWOAK over MapReduce-programming
model. The parallel-computing version is called as MapReduce-based TSWOAK (MR-TSWOAK). MR-
TSWOAK works in two phases, MR-TSWOAK-Map and MR-TSWOAK-Reduce. The data-points are
distributed uniformly among Hadoop Distributed File System (HDFS) \cite{a12} DataNodes. MR-TSWOA-Map
processes data-point and finds cluster in each whale, which would have the least Euclidean distance
between the data-point and found centroid. The output of the Map phase is {key:(whaleId, cenId),
value:minDistance}, where ‘whaleId’ is the identification of whale of whose clusters are being matched
with the data-point.,‘cenId’ is the identification of cluster-centroid whose Euclidean distance is minimumwith the data-point. ‘minDistance’ is the Euclidean distance between data-point and the centroid with
identification ‘cenId’. The pseudo-code of Map phase is given in Algorithm \ref{algo:Map}. MR-TSWOAK-Reduce
phase processes the distance given by Map phase, and calculate intra-cluster distance for each centroid of
each whale. The output of this phase is of the form {key:(whaleId, cenId), value:intra_cluster_distance}.
The psudo-code of Reduce phase is given in Algorithm \ref{algo:Reduce}. The architecture of MR-TSWOAK is given in Fig. \ref{fig:TSWOAK}.   

      \begin{figure}
    \centering
     \includegraphics\cite{awidth=1.0\textwidth}{Figure1}
      \caption{Architecture of MR-TSWOAK}
\label{fig:TSWOAK}
\end{figure}

\begin{algorithm}
\caption{ MR-TSWOAK-Map}
\small
\label{algo:Map}
\begin{algorithmic}
  \FOR{each whale in population}
    \STATE whaleId=ID_of_whale
    \STATE centroidArray=retrieve_centroid_array_of_given_whale(whaleID)
    \STATE minDistance=INT_MAX
    \FOR{each centroid in centroidArray}
      \STATE distance=Euclidian_distance(data-point,centroid)
      \IF{distance < minDistance}
        \STATE centId=ID_of_centroid
        \STATE minDistance=distance
      \ENDIF
    \ENDFOR
    \STATE key=(whaleID,centID)
  \ENDFOR
  \STATE return \{key:(whaleID,centroidID),value:minDistance\} 
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{ MR-TSWOAK-Reduce}
\small
\label{algo:Reduce}
\begin{algorithmic}  
\STATE\textbf{Input:} \{key:(whaleID,centroidID),value-list:distances\}
\STATE intra-cluster-distance=0
\FOR{each distance in distances}
  \STATE intra-cluster-distance+=distance
\ENDFOR
\STATE return \{key:(whaleID,centID),value:intra-cluster-distance\}
\end{algorithmic}
\end{algorithm}

\subsection{Application of MR-TSWOAK on Product Recommendation System}\label{sec:recomSys}
The recommendation matrix is stored on the Hadoop Distributed File System (HDFS). The
recommendation matrix has dimension of (users, product). The number of rows of matrix is equal to number
of users. The number of columns is equal to number of products. Each row of matrix is a datapoint to MR-
TSWOAK. The MR-TSWOAK is implemented over the recommendation matrix, and it output the desired
solution of centroid position vector. Further this position vector is fine-tuned by running K-Means over it.
The system is tested by calculating the Mean Absolute Error


\section{Experimental results }\label{sec:ep}
Evaluation of proposed method is done in three phases. In the first phase, TSWOAK is tested on seven UCI
datasets and the results are compared on the basis of intra-cluster distance with five state of the art meta-
heuristic algorithms namely PSO, K-Means, GSA, BA, GWO. The seven UCI datasets are Iris, Seeds,
Glass, Cancer, Balance, Haberman, and Wine. In the second phase, the MapReduce based method MR-
TSWOAK is tested on four extremely large datasets namely Replicated CMC, Replicated Vowel,Replicated Iris, Replicated Wine, and the results are compared to Parallel K-PSO, DFBKBA, Parallel K-
Means, MR-ABC on the basis of F-measure and computation time. In the third phase, the proposed method
MR-TSWOAK is tested for the applicability in recommender systems. For this it is tested on MovieLens
dataset, and its results will be compared on the basis of Mean Absolute Error (MAE) with the results of
already present algorithms namely, K-Means, PCA-K-Means, ABC-KM, GAKM, PCA-GAKM, SOM,
PCA-SOM, UPCC.

\subsection{Experimental setup}  
The experiments related to TSWOAK were done on a single machine, with specification as 2.5 GHz Intel
i5 processor, 8GB RAM, and 1 TB hard-disk.
For experiments related to the MR-TSWOAK, Hadoop cluster of capacity of five nodes was made. Each
node had specification as 2.5 GHz Intel i5 processor, 8 GB RAM, 1 TB hard-disk. Java version used was
openjdk “1.8.0_191”. The version of Hadoop was 2.6.0.

\subsection{Performance analysis of TSWOAK based clustering}\label{sec:expr1}
The proposed method TSWOA was tested on seven UCI datasets namely, Iris, Seeds, Glass, Cancer,
Balance, Haberman, and Wine, and the results were compared on the basis of intra-cluster distance with
five popular meta-heuristic algorithms namely, PSO, K-Means, GSA, BA, and GWO. Description of seven
datasets used is given in Table 1. The setting parameters of each meta-heuristic algorithm used for
experimentation are described in Table 2. The intra-cluster distance found by each meta-heuristic algorithm
on each dataset is given in Table 3.

Table 3 shows that the proposed algorithm TSWOAK gives the minimum intra-cluster distance with respect
to other algorithms used in experimentation.
The convergence speed of the proposed algorithm TSWOAK is more than the convergence speed of other
algorithms with which the comparison has been done. Figure 2 and figure 3 shows the convergence graph
of the algorithms on Wine and Glass datasets respectively. In both the diagram, the Y-axis depicts the bestintra-cluster distance for an algorithm, the X-axis depicts the number of iterations for which the algorithm
has been run.

\begin{table}
\caption{Dataset Description}
\scriptsize
\begin{center}
\renewcommand{\arraystretch}{.75}
  \begin{tabular}{l l l l}
   
    \hline
\textbf{Dataset} & \textbf{Number of Datapoints} & \textbf{Number of Attributes} &\textbf{Number of Clusters}    \\

\hline
Balance	      &	625	&	 4   &	3	\\
Cancer	&	638	&	9	&	2	\\
Iris	&	150	&	4	&	3	\\
Haberman	&	306	&	9	&	214	\\
Wine	&	178	&	13	&	3	\\
Glass	&	214	&	9	&	6	\\
Seeds	&	210	&	7	&	3	\\
 
  \end{tabular}
\end{center}
\label{tab:dataset}
\end{table}

\begin{table}
\caption{Setting parameters of each meta-heuristic algorithm for experimentation}
\scriptsize
\begin{center}
\renewcommand{\arraystretch}{0.7}
  \begin{tabular}{l l l l l l l}
    \hline
    \hline
\textbf{Parameter Name} & \textbf{K-Means} &\textbf{BA} &\textbf{PSO} &\textbf{GWO}& \textbf{GSA} &\textbf{TSWOAK}\\
\hline
Population size (pop)	&	--& 40	&	40 &	40	&	40	&40\\
Congnitive Constant (c1)  & --& -- & 1& --  & --&-- \\
r0  & --& 0.9& --& --  & --&--\\
gamma ($\gamma$)& --& 0.9& --& --& --&-- \\
Alpha ($\alpha$) & 20& 2&  -- & 2& 0.9 &--\\
fmin  & --& 0& --& -- & --&--\\
fmax  & --& 2& --&  & --&-- \\
Inertial Constant (w)	&	--& --& 0.5&	--	&	--&--	\\
Social Constant (c2)	&--& --& 1&	--	&	--&--\\
G-constant (G0)&--& --& --& --&	20&--\\
Number of Iterations (itr) &  500 & 500 & 500 & 500 & 500 & 500\\
    \hline
  \end{tabular}
\end{center}
\label{tab:Param}
\end{table}
   

\begin{table}
\caption{Mean intra-cluster distance for 30 iteration of each algorithm on each dataset}
\scriptsize
\begin{center}
\renewcommand{\arraystretch}{1}
  \begin{tabular}{l l l l l l l r}
    
    \hline
\textbf{Dataset} & \textbf{Criteria} 		&		\textbf{K-Means}     &	 \textbf{PSO}  &\textbf{GSA} &\textbf{BA}&\textbf{GWO}&\textbf{EGWO} \\
\hline
Iris	&	Best	&	97.34084	&	96.78998	&	96.65548	&	96.65552	&	96.65826	&	\textbf{96.65548}	\\	
&	Mean	&	106.33437	&	97.13691	&	96.67516	&	99.53097	&	\textbf{99.12574}	&	99.55645	\\
Seeds	&	Best	&	587.31957	&	312.68370	&	311.79804&	311.79816	&	311.88200	&	\textbf{311.79804}	\\	
&	Mean	&	588.10457&	313.85971	&	311.79804	&	315.41951	&	312.09220	&	\textbf{311.79804}	\\
Glass	&	Best	&	292.75724	&	238.51144	&	286.11855	&	243.70331&	265.81420	&	\textbf{214.44399}	\\	
&	Mean	&	325.54765	&	257.06514	&	316.71044	&	264.10417	&	302.04114	&	\textbf{242.68894}	\\
Cancer	&	Best	&	19323.17382	&	2969.23958&	2970.17834	&	2964.38718	&	2964.390179	&	\textbf{2964.38697}	\\	
&	Mean	&	19323.17693	&	2976.15128&	2994.77937	&	3032.42259	&	2964.39495	&	\textbf{2964.38697}	\\
Balance	&	Best	&	3472.32142	&	1423.96787	&	1423.82042&	1424.04307	&	1423.82106&	\textbf{1423.82040}	\\	
&	Mean	&	3493.80000	&	1424.62818	&	1424.51503	&	1426.28547	&	\textbf{1423.82963}	&	1424.20479	\\
Haberman	&	Best	&	30507.02076	&	2566.99548&	2566.98989	&	2566.98889	&	2567.02562	&	\textbf{2566.98889}	\\
	&	Mean	&	32271.96242	&	\textbf{2567.12294}	&	2582.08625	&	2648.88585	&	2590.77309	&	2637.34900	\\
Wine	&	Best	&	2370689.68700	&	16298.98906	&	17038.59226	&	16371.05448	&	16307.09242	&	\textbf{16292.18465}	\\	
&	Mean	&	2484626.08700	&	16305.11720	&	17709.43544	&	16865.72325	&	16318.41351	&	\textbf{16292.35069}	\\

    \hline
  \end{tabular}
\end{center}
\label{tab:result}
\end{table}


 \begin{figure}
    \centering
     \includegraphics\cite{awidth=1.0\textwidth}{Wine}
      \caption{Convergence graph of Wine}
\label{fig:Wine}
\end{figure}

 \begin{figure}
    \centering
     \includegraphics\cite{awidth=1.0\textwidth}{Glass}
      \caption{Convergence graph of Glass}
\label{fig:Glass}
\end{figure}

\subsection{Performance analysis of MapReduce Based EGWO (MR-EGWO)}\label{sec:expr2}
Section 4.2 have shown that the proposed algorithm TSWOAK has given better result in comparison to
other algorithms. Hence, the authors have proposed a MapReduce-based version of TSWOAK called as
MR-TSWOAK to handle extremely large datasets. The performance of MR-TSWOAK is compared with
other state of art MapReduce-based meta-heuristic algorithms on the basis of F-Measure and computation
time. Table 4 describes the large datasets to be used. Table 5 gives the results of different algorithms on
different dataset on the basis of F-measure and computation time.
Table 5 shows that the performance of MR-TSWOAK is better than other algorithms, but the parallel K-
Means is computationally fast.Speedup-performance is also a major criteria to evaluate a parallel-computing algorithm. The calculation
of speedup-performance is done according to Eq. (\ref{eq:11}).
  \begin{equation}\label{eq:11}                                                            			
       S_p=T_{base}/T_N                                                     		
   \end{equation}
Where $T_{base}$ is time taken by algorithm to run on one machine. $T_N$ is time taken by the algorithm to run on
N machines. Speedup performance of MR-TSWOAK is studied on Replicated CMC and Replicated Iris
datasets. Figure 4 and 5 are the speedup graph of MR-TSWOAK running on Replicated CMC and
Replicated Iris datasets. The speedup graph has Y-axis for computation time, and X-axis for the number of
nodes in the cluster. The speedup performance of MR-TSWOAK running on CMC dataset is 4.7548, when
there are five nodes in the cluster. The speedup performance of MR-TSWOAK running on Replicated Iris
dataset is 4.4561, when there are five nodes in the cluster. This shows that MR-TSWOAK can be used on
large datasets.

\begin{table}
\caption{Description of Large Datasets}
\scriptsize
\begin{center}
\renewcommand{\arraystretch}{0.7}
  \begin{tabular}{l l l l }

    \hline
\textbf{Dataset} & \textbf{\#C} & \textbf{\#D} 		&		\textbf{\#N}    \\
\hline
 Replicated Iris	&	3	&	7	&	10,000,050	\\
Replicated CMC	&	3	&	9	&	10,000,197	\\
Replicated Wine	&	2	&	18	&	5000000	\\
Replicated Vovel &	10	& 10	&	1025010	\\

    \hline
  \end{tabular}
\end{center}
\label{tab:large dataset}
\end{table}


\begin{table}
\caption{F-measure and computation time for each algorithm on each dataset on 30 iterations}
\scriptsize
\begin{center}
\renewcommand{\arraystretch}{0.7}
  \begin{tabular}{l l l l l l l}
    \hline
    \hline
\textbf{Data set} & \textbf{PK-Means} &\textbf{parallel KPSO} &\textbf{MR-ABC} &\textbf{DFBPKBA} &\textbf{MR-EGWO}\\
\hline

1	&	0.667	&	0.785	&	0.842	&	0.790  &  0.846	\\
2	&	0.298	&	0.324	&	0.387	&	0.378  &  0.391	\\
3	&	0.482	&	0.517	&	0.718	&	0.719  &  0.733	\\
4	&	0.586	&	0.627	&	0.634	&	0.622  &  0.635	\\

    \hline
  \end{tabular}
\end{center}
\label{tab:FM}
\end{table}



\begin{figure}
    \centering
     \includegraphics\cite{awidth=1.0\textwidth}{Glass}
      \caption{Speedup graph of Replicated-CMC dataset}
\label{fig:CMC}
\end{figure}

\begin{figure}
    \centering
     \includegraphics\cite{awidth=1.0\textwidth}{Glass}
      \caption{Speedup graph of Replicated-Iris dataset}
\label{fig:Iris}
\end{figure}
 
\subsection{Performance analysis of MapReduce Based EGWO (MR-EGWO)}
The recommendation system developed over MR-TSWOA is tested on MovieLens dataset, which contains
100,000 datapoints and has ratings given by users to different movies. There are 1000 users in the dataset
and 1700 movies in the dataset. Each user has given rating to at least 20 movies. The results of the test are
compared with the results of PCA-GAKM, PCA-SOM, SOM, UPCC, K-Means, PCA-K-Means, GAKM,
ABC-KM on the basis of Mean Absolute Error (MAE). The results are given in Table 6.
Table 6 shows that MR-TSWOAK perform better than the already present collaborative-filtering clustering
algorithm. Figure 6 shows the comparison of MAE with different methods.

\begin{figure}
    \centering
     \includegraphics\cite{awidth=1.0\textwidth}{Glass}
      \caption{Comparison of MAE with different methods}
\label{fig:MAE}
\end{figure}

\section{Conclusion}\label{sec:con}

In the 21 st century, it is very important for e-commerce business to assist their customers into buying better
product for themselves. For this kind of assistance, recommendation systems have come into picture and
quite popular these days. Collaborative-filtering is a popular approach for recommendation. K-means is a
popular algorithm used for collaborative-filtering. Though recommendation systems have been successful
in assisting customers, but still they suffer from the problem of getting into local-optima, and have
scalability problems. To overcome these problems, the authors have proposed a method that uses the
hunting behaviors of Humpback whales and the concept of tournament selection. This method is called as
Tournament Selection empowered Whale Optimization Algorithm optimized K-Means (TSWOAK). This
method is tested on seven benchmark UCI datasets namely, Haberman, Cancer, Iris, Glass, Wine, Balance,
Seeds, and the results are compared with the results of start of art algorithms namely, K-Means, BA, PSO,
GWO, GSA, on the basis of intra-cluster distances. The comparison has shown that the proposed method
works better than already present clustering algorithms. To solve the problem of scalability, the authors
have adopted TSWOAK on MapReduce programming model, called MR-TSWOAK. The MapReduce-
based model MR-TSWOAK was tested on four large datasets namely, Reproduced CMC, Reproduced
Vowel, Reproduced Iris, Reproduced Wine, and the results are compared with the results of Parallel K-
PSO, DFBPKBA, Parallel K-Means, MR-ABC, on the basis of F-Measure and Computation time. The
comparison has shown that the F-measure found by MR-TSWOAK is better than those by other algorithms.
The comparison has implied that the MapReduce-based method can be used for clustering purpose. The
authors have further tested MR-TSWOAK on MovieLens dataset to check for Recommendation System
applicability. The results were compared to the result found by state of art collaborative-filtering algorithms
namely, PCA-GAKM, PCA-SOM, SOM, UPCC, K-Means, PCA-K-Means, GAKM, ABC-KM on the
basis of Mean Absolute Error (MAE). The comparison has shown that the MAE found by MR-TSWOAK
is lower than MAE found by other algorithms. The comparison has implied that the proposed method MR-
TSWOAK can be used for Recommendation Systems application.
In future, Tournament Selection empowered Whale Optimization Algorithm shall be used to optimize the
weight of Convolution Neural Network, which would be implemented over MapReduce. This shall be done
to check if unsupervised learning is better or supervised learning is better for the product recommendation.\\
\textbf{References}



\bibliographystyle{elsarticle-num}
\begin{thebibliography}{10}
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
\expandafter\ifx\csname href\endcsname\relax
  \def\href#1#2{#2} \def\path#1{#1}\fi

\bibitem{a1}
Liu, Haifeng, Xiangjie Kong, Xiaomei Bai, Wei Wang, Teshome Megersa Bekele, and Feng Xia.
"Context-based collaborative filtering for citation recommendation." IEEE Access 3 (2015): 1695-
1703.

\bibitem{a2}
Bobadilla, Jesús, Fernando Ortega, Antonio Hernando, and Abraham Gutiérrez. "Recommender
systems survey." Knowledge-based systems 46 (2013): 109-132.

\bibitem{a3}
Lam, Xuan Nhat, Thuc Vu, Trong Duc Le, and Anh Duc Duong. "Addressing cold-start problem
in recommendation systems." In Proceedings of the 2nd international conference on Ubiquitous
information management and communication, pp. 208-211. ACM, 2008.

\bibitem{a4}
Ungar, Lyle H., and Dean P. Foster. "Clustering methods for collaborative filtering." In AAAI
workshop on recommendation systems, vol. 1, pp. 114-129. 1998

\bibitem{a5}
Tripathi, Ashish Kumar, Kapil Sharma, and Manju Bala. "A novel clustering method using
enhanced grey wolf optimizer and mapreduce." Big data research 14 (2018): 93-100.

\bibitem{a6}
Hatamlou, Abdolreza, Salwani Abdullah, and Hossein Nezamabadi-Pour. "Application of
gravitational search algorithm on data clustering." In International Conference on Rough Sets and
Knowledge Technology, pp. 337-346. Springer, Berlin, Heidelberg, 2011.
\bibitem{a7}
Maulik, Ujjwal, and Sanghamitra Bandyopadhyay. "Genetic algorithm-based clustering
technique." Pattern recognition 33, no. 9 (2000): 1455-1465.

\bibitem{a8}
Ashish, Tripathi, Sharma Kapil, and Bala Manju. "Parallel bat algorithm-based clustering using
mapreduce." In Networking Communication and Data Knowledge Engineering, pp. 73-82.
Springer, Singapore, 2018.

\bibitem{a9}
Pandey, Avinash Chandra, Dharmveer Singh Rajpoot, and Mukesh Saraswat. "Twitter sentiment
analysis using hybrid cuckoo search method." Information Processing & Management 53, no. 4
(2017): 764-779.

\bibitem{a10}
Alam, Shafiq, Gillian Dobbie, and Patricia Riddle. "Particle swarm optimization based clustering
of web usage data." In Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web
Intelligence and Intelligent Agent Technology-Volume 03, pp. 451-454. IEEE Computer Society,
2008.
\bibitem{a11}
Dean, Jeffrey, and Sanjay Ghemawat. "MapReduce: simplified data processing on large clusters."
Communications of the ACM 51, no. 1 (2008): 107-113.

\bibitem{a12}
Shvachko, Konstantin, Hairong Kuang, Sanjay Radia, and Robert Chansler. "The hadoop
distributed file system." In 2010 IEEE 26th symposium on mass storage systems and technologies
(MSST), pp. 1-10. Ieee, 2010.
\bibitem{a13}
Banharnsakun, Anan. "A MapReduce-based artificial bee colony for large-scale data clustering."
Pattern Recognition Letters 93 (2017): 78-84.

\bibitem{a14}
Tripathi, Ashish Kumar, Kapil Sharma, and Manju Bala. "Dynamic frequency based parallel k-bat
algorithm for massive data clustering (DFBPKBA)." International Journal of System Assurance
Engineering and Management 9, no. 4 (2018): 866-874.

\bibitem{a15}
Mirjalili, Seyedali, and Andrew Lewis. "The whale optimization algorithm." Advances in
engineering software 95 (2016): 51-67.

\bibitem{a16}
Mafarja, Majdi M., and Seyedali Mirjalili. "Hybrid Whale Optimization Algorithm with simulated
annealing for feature selection." Neurocomputing 260 (2017): 302-312.

\bibitem{a17}
Miller, Brad L., and David E. Goldberg. "Genetic algorithms, tournament selection, and the effects
of noise." Complex systems 9, no. 3 (1995): 193-212.





\end{thebibliography}



\end{document}











